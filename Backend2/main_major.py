# -*- coding: utf-8 -*-
"""MAIN MAJOR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1co5Y51_0cOgUYEn_LFSdqvJQO1zFHRrb
"""

# Fix all version conflicts (run this once)
!pip install -U pip
!pip uninstall -y transformers huggingface_hub
!pip install transformers==4.40.2 sentence-transformers==2.2.2 huggingface_hub==0.24.6 --quiet

from sentence_transformers import SentenceTransformer

# STEP 1 â€” Setup
!pip install sentence-transformers --quiet

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from sklearn.ensemble import IsolationForest
import re
from google.colab import files

print("âœ… Libraries imported successfully.")

# STEP 2 â€” Upload chat file
uploaded = files.upload()

filename = list(uploaded.keys())[0]
with open(filename, "r", encoding="utf-8") as f:
    lines = f.readlines()

print("âœ… File uploaded. Total messages:", len(lines))
print("Example lines:\n", lines[:5])

# STEP 3 â€” Clean and extract users + messages
data = []
for line in lines:
    match = re.match(r"\[.*?\]\s*(.*?):\s*(.*)", line.strip())
    if match:
        user, msg = match.groups()
        data.append((user, msg))

df = pd.DataFrame(data, columns=["user", "message"])
print("âœ… Parsed messages:")
df.head()

# STEP 4 â€” Generate embeddings and a summary of the main topic
model = SentenceTransformer('all-MiniLM-L6-v2')

# Embed all messages
embeddings = model.encode(df['message'].tolist())

# Compute mean vector (represents main discussion)
centroid = np.mean(embeddings, axis=0)

# Find the 5 most similar messages (most representative of the main topic)
sims = cosine_similarity([centroid], embeddings)[0]
df['similarity_to_main'] = sims

main_topic_msgs = df.sort_values('similarity_to_main', ascending=False).head(5)['message'].tolist()
summary_text = " ".join(main_topic_msgs)
print("ðŸ§¾ Summary of Main Chat Discussion (automatically extracted):\n")
print(summary_text)

# STEP 5 â€” Flag outliers (messages different from the main topic)
# Compute similarity to the centroid (already done)
df['zscore'] = (df['similarity_to_main'] - df['similarity_to_main'].mean()) / (df['similarity_to_main'].std() + 1e-9)

# Threshold for being "different"
threshold = -1.0  # lower = more different
df['is_different'] = (df['zscore'] < threshold).astype(int)

# Show only messages that differ significantly
outliers = df[df['is_different'] == 1].sort_values('similarity_to_main')
print(f"âš ï¸ Found {len(outliers)} messages that differ from the main chat context.\n")
outliers[['user', 'message', 'similarity_to_main']].head(15)



# STEP 6 â€” Save outlier messages
outliers.to_csv("different_messages.csv", index=False)
print("ðŸ“ Saved different messages to 'different_messages.csv'")

# To download in Colab:
files.download("different_messages.csv")

!pip install -U pip
!pip uninstall -y transformers huggingface_hub -q
!pip install transformers==4.40.2 sentence-transformers==2.2.2 huggingface_hub==0.24.6 --quiet

import pandas as pd
import numpy as np
import re
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from google.colab import files

uploaded = files.upload()

filename = list(uploaded.keys())[0]
with open(filename, "r", encoding="utf-8") as f:
    lines = f.readlines()

print("âœ… File uploaded. Total lines:", len(lines))

data = []
for line in lines:
    match = re.match(r"\[.*?\]\s*(.*?):\s*(.*)", line.strip())
    if match:
        user, msg = match.groups()
        data.append((user.strip(), msg.strip()))

df = pd.DataFrame(data, columns=["user", "message"])
print("âœ… Parsed messages:")
df.head()

model = SentenceTransformer('all-MiniLM-L6-v2')

embeddings = model.encode(df['message'].tolist())
centroid = np.mean(embeddings, axis=0)

# Cosine similarity â†’ closeness to main topic
df['similarity_to_main'] = cosine_similarity([centroid], embeddings)[0]

# Top 5 most representative messages
summary_msgs = df.sort_values('similarity_to_main', ascending=False).head(5)['message'].tolist()
summary_text = " ".join(summary_msgs)

print("ðŸ§¾ Auto-summary of main discussion:\n")
print(summary_text)

from scipy.special import expit  # sigmoid

df['dissimilarity_index'] = 1 - df['similarity_to_main']
mean_sim = df['similarity_to_main'].mean()
std_sim = df['similarity_to_main'].std() + 1e-9
df['zscore'] = (df['similarity_to_main'] - mean_sim) / std_sim

# Suspicion confidence â€” scaled sigmoid
df['suspicion_confidence'] = expit(-df['zscore'] * 1.5)

# Flag suspicious if below threshold
df['is_suspicious'] = (df['suspicion_confidence'] > 0.7).astype(int)

sus_df = df[df['is_suspicious'] == 1].sort_values('suspicion_confidence', ascending=False)
print(f"âš ï¸ Total suspicious messages found: {len(sus_df)}\n")
sus_df[['user','message','similarity_to_main','dissimilarity_index','zscore','suspicion_confidence']].head(10)

top_users = sus_df['user'].value_counts().reset_index()
top_users.columns = ['user', 'suspicious_message_count']
print("\nðŸ‘¤ Top users with suspicious messages:\n")
print(top_users.head(10))

sus_df.to_csv("suspicious_messages_detailed.csv", index=False)
files.download("suspicious_messages_detailed.csv")

print("\nðŸ“ Saved detailed suspicious message analysis.")

!pip install matplotlib seaborn plotly --quiet

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

df.columns

# Example fix
if "label" not in df.columns:
    if "predicted_label" in df.columns:
        df = df.rename(columns={"predicted_label": "label"})
    elif "is_suspicious" in df.columns:
        df = df.rename(columns={"is_suspicious": "label"})
    else:
        # If you have no label column, create one from confidence threshold
        df["label"] = (df["suspicion_confidence"] > 0.5).astype(int)

plt.figure(figsize=(8,6))
sns.scatterplot(
    data=df,
    x="dissimilarity_index",
    y="suspicion_confidence",
    hue="label",
    palette={0: "green", 1: "red"},
    alpha=0.7
)
plt.title("Dissimilarity vs Suspicion Confidence")
plt.xlabel("Dissimilarity Index (0 = similar, higher = more unusual)")
plt.ylabel("Suspicion Confidence (0 to 1)")
plt.legend(title="Message Type", labels=["Normal","Suspicious"])
plt.show()

plt.figure(figsize=(8,5))
sus_users = df[df["label"]==1]["user"].value_counts().head(10)
sns.barplot(x=sus_users.values, y=sus_users.index, palette="Reds_r")
plt.title("Top Users with Suspicious Messages")
plt.xlabel("Count of Suspicious Messages")
plt.ylabel("User")
plt.show()

plt.figure(figsize=(8,5))
sns.histplot(df["dissimilarity_index"], bins=20, kde=True, color="purple")
plt.title("Distribution of Dissimilarity Index")
plt.xlabel("Dissimilarity Index")
plt.ylabel("Frequency")
plt.show()

fig = px.scatter(
    df,
    x="dissimilarity_index",
    y="suspicion_confidence",
    color=df["label"].map({0:"Normal",1:"Suspicious"}),
    size="suspicion_confidence",
    hover_data=["user","message"],
    title="Interactive Suspicious Message Detection Overview"
)
fig.show()

summary = df.groupby("user").agg({
    "label":"sum",
    "dissimilarity_index":"mean",
    "suspicion_confidence":"mean"
}).rename(columns={
    "label":"Suspicious_Message_Count",
    "dissimilarity_index":"Avg_Dissimilarity",
    "suspicion_confidence":"Avg_Confidence"
}).sort_values(by="Suspicious_Message_Count", ascending=False)

display(summary.head(10))